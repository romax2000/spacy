{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET determiner\n",
      "firm NOUN noun\n",
      "earned VERB verb\n",
      "$ SYM symbol\n",
      "1.5 NUM numeral\n",
      "million NUM numeral\n",
      "in ADP adposition\n",
      "2017 NUM numeral\n",
      ". PUNCT punctuation\n"
     ]
    }
   ],
   "source": [
    "#Numeric, Symbolic, and Punctuation Tags\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u\"The firm earned $1.5 million in 2017.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET DT determiner\n",
      "firm NOUN NN noun, singular or mass\n",
      "earned VERB VBD verb, past tense\n",
      "$ SYM $ symbol, currency\n",
      "1.5 NUM CD cardinal number\n",
      "million NUM CD cardinal number\n",
      "in ADP IN conjunction, subordinating or preposition\n",
      "2017 NUM CD cardinal number\n",
      ". PUNCT . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1.5 million\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u\"The firm earned $1.5 million in 2017.\")\n",
    "phrase = ''\n",
    "for token in doc:\n",
    "    if token.tag_ == '$':\n",
    "        phrase = token.text\n",
    "        i = token.i+1\n",
    "        while doc[i].tag_ == 'CD':\n",
    "            phrase += doc[i].text + ' '\n",
    "            i += 1\n",
    "        break\n",
    "phrase = phrase[:-1]\n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1.5 million\n",
      "$1.2 million\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u\"The firm earned $1.5 million in 2017, in comparison with $1.2 million in 2016.\")\n",
    "phrase = ''\n",
    "for token in doc:\n",
    "    if token.tag_ == '$':\n",
    "        phrase = token.text\n",
    "        i = token.i+1\n",
    "        while doc[i].tag_ == 'CD':\n",
    "            phrase += doc[i].text + ' '\n",
    "            i += 1\n",
    "        phrase = phrase[:-1]\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON PRP\n",
      "can VERB MD\n",
      "promise VERB VB\n",
      "it PRON PRP\n",
      "is AUX VBZ\n",
      "worth ADJ JJ\n",
      "your DET PRP$\n",
      "time NOUN NN\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "#Turning Statements into Questions\n",
    "doc = nlp(u\"I can promise it is worth your time.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you really promise it is worth my time?\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u\"I can promise it is worth your time.\")\n",
    "sent = ''\n",
    "for i,token in enumerate(doc):\n",
    "    if token.tag_ == 'PRP' and doc[i+1].tag_ == 'MD' and doc[i+2].tag_ == 'VB':\n",
    "        sent = doc[i+1].text.capitalize() + ' ' + doc[i].text\n",
    "        sent = sent + ' ' + doc[i+2:].text\n",
    "        break\n",
    " #By now, you should have: 'Can I promise it is worth your time.'\n",
    "   #Retokenization\n",
    "doc=nlp(sent)\n",
    "for i,token in enumerate(doc):\n",
    "    if token.tag_ == 'PRP' and token.text == 'I':\n",
    "        sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
    "        break\n",
    " #By now, you should have: 'Can you promise it is worth your time.'\n",
    "doc=nlp(sent)\n",
    "for i,token in enumerate(doc):\n",
    "    if token.tag_ == 'PRP$' and token.text == 'your':\n",
    "        sent = doc[:i].text + ' my ' + doc[i+1:].text\n",
    "        break\n",
    " #By now, you should have: 'Can you promise it is worth my time.' \n",
    "doc=nlp(sent)\n",
    "for i,token in enumerate(doc):\n",
    "    if token.tag_ == 'VB':\n",
    "        sent = doc[:i].text + ' really ' + doc[i:].text\n",
    "        break\n",
    " #By now, you should have: 'Can you really promise it is worth my time.'\n",
    "doc=nlp(sent)\n",
    "sent = doc[:len(doc)-1].text + '?'\n",
    " #Finally, you should have: 'Can you really promise it is worth my time?'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON PRP nsubj nominal subject\n",
      "can VERB MD aux auxiliary\n",
      "promise VERB VB ROOT None\n",
      "it PRON PRP nsubj nominal subject\n",
      "is AUX VBZ ccomp clausal complement\n",
      "worth ADJ JJ acomp adjectival complement\n",
      "your DET PRP$ poss possession modifier\n",
      "time NOUN NN npadvmod noun phrase as adverbial modifier\n",
      ". PUNCT . punct punctuation\n"
     ]
    }
   ],
   "source": [
    "#Distinguishing Subjects from Objects\n",
    "doc = nlp(u\"I can promise it is worth your time.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, token.dep_, spacy.explain(token.dep_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do I want a blue one?\n"
     ]
    }
   ],
   "source": [
    "#Deciding What Question a Chatbot Should Ask\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "def find_chunk(doc):\n",
    "    chunk = ''\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.dep_ == 'dobj':\n",
    "            shift = len([w for w in token.children])\n",
    "            #print([w for w in token.children])\n",
    "            chunk = doc[i-shift:i+1]\n",
    "            break\n",
    "    return chunk\n",
    "\n",
    "def determine_question_type(chunk):\n",
    "    question_type = 'yesno'\n",
    "    for token in chunk:\n",
    "        if token.dep_ == 'amod':\n",
    "            question_type = 'info'\n",
    "    return question_type\n",
    "\n",
    "def generate_question(doc, question_type):\n",
    "    sent = ''\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.tag_ == 'PRP' and doc[i+1].tag_ == 'VBP':\n",
    "            sent = 'do ' + doc[i].text\n",
    "            sent = sent + ' ' + doc[i+1:].text\n",
    "            break\n",
    "    doc=nlp(sent)\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.tag_ == 'PRP' and token.text == 'I':\n",
    "            sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
    "            break\n",
    "        doc=nlp(sent)\n",
    "        if question_type == 'info':\n",
    "            for i,token in enumerate(doc):\n",
    "                if token.dep_ == 'dobj':\n",
    "                    sent = 'why ' + doc[:i].text + ' one ' + doc[i+1:].text\n",
    "                    break\n",
    "        if question_type == 'yesno':\n",
    "            for i,token in enumerate(doc):\n",
    "                if token.dep_ == 'dobj':\n",
    "                    sent = doc[:i-1].text + ' a red ' + doc[i:].text\n",
    "                    break\n",
    "        doc=nlp(sent)\n",
    "        sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'\n",
    "        return sent\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp('I want a blue apple.')\n",
    "    chunk = find_chunk(doc)\n",
    "    if str(chunk) == '':\n",
    "        print('The sentence does not contain a direct object.')\n",
    "        sys.exit()\n",
    "    question_type = determine_question_type(chunk)\n",
    "    question = generate_question(doc, question_type)\n",
    "    print(question)\n",
    "else:\n",
    "    print('You did not submit a sentence!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
