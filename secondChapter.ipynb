{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this this\n",
      "product product\n",
      "integrates integrate\n",
      "both both\n",
      "libraries library\n",
      "for for\n",
      "downloading download\n",
      "and and\n",
      "applying apply\n",
      "patches patch\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'this product integrates both libraries for downloading and applying patches')\n",
    "for token in doc:\n",
    "  print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'flying', 'to', 'Minsk']\n",
      "['-PRON-', 'be', 'fly', 'to', 'Moskau']\n"
     ]
    }
   ],
   "source": [
    "#Applying Lemmatization for Meaning Recognition\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'I am flying to Minsk')\n",
    "print([w.text for w in doc])\n",
    "special_case = [{ORTH: u'Minsk', LEMMA: u'Moskau'}]\n",
    "nlp.tokenizer.add_special_case(u'Minsk', special_case)\n",
    "print([w.lemma_ for w in nlp(u'I am flying to Minsk')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flying']\n",
      "['LA', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "#Using Part-of-Speech Tags to Find Relevant Verbs\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
    "print([w.text for w in doc if w.tag_== 'VBG' or w.tag_== 'VB'])\n",
    "print([w.text for w in doc if w.pos_ == 'PROPN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj\n",
      "have AUX aux\n",
      "flown VERB ROOT\n",
      "to ADP prep\n",
      "LA PROPN pobj\n",
      ". PUNCT punct\n",
      "Now ADV advmod\n",
      "I PRON nsubj\n",
      "am AUX aux\n",
      "flying VERB ROOT\n",
      "to ADP prep\n",
      "Frisco PROPN pobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "#Syntactic Relations\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
    "for token in doc:\n",
    "  print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flown nsubj I\n",
      "flown aux have\n",
      "flown ROOT flown\n",
      "flown prep to\n",
      "to pobj LA\n",
      "flown punct .\n",
      "flying advmod Now\n",
      "flying nsubj I\n",
      "flying aux am\n",
      "flying ROOT flying\n",
      "flying prep to\n",
      "to pobj Frisco\n",
      "flying punct .\n"
     ]
    }
   ],
   "source": [
    "#When you print this line, youâ€™ll see how words in the discourse sentences \n",
    "#are connected to each other by syntactic dependencies.\n",
    "for token in doc:\n",
    "  print(token.head.text, token.dep_, token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flown', 'LA']\n",
      "['flying', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "#The following script locates words that are assigned to those two dependency labels:\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
    "for sent in doc.sents:\n",
    "    print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fly', 'San', 'Francisco']\n"
     ]
    }
   ],
   "source": [
    "print([w.lemma_ for w in nlp(u'I am flying to San Francisco') if w.dep_ == 'ROOT' or w.dep_ == 'pobj' or (w.dep_ == 'compound' and w.head.dep_ =='ROOT' or w.head.dep_ == 'pobj')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA GPE\n",
      "Frisco ORG\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recognition\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
